Wireless networks of biologically inspired distributed sensors (BIDS) are hypothesized to enable improved overall detection accuracy using ultra-low power and low bandwidth spike-based communication between nodes. Unlike traditional sensor networks, in which nodes communicate via digital protocols that require precise decoding of binary signal packets, BIDS nodes communicate by broadcasting generic radio frequency pulses, or spikes. Individual BIDS nodes are modeled after leaky integrate-and-fire (LIF) neurons, in which both filtered sensory signals and inputs from other BIDS nodes are accumulated as capacitive charge that decays with a characteristic time constant. A BIDS node itself broadcasts a spike whenever its internal state exceeds a threshold value. Here we present detailed simulations of a BIDS network designed to detect a moving target - modeled as a pure acoustic tone with a translating origin - against a background of 1/f noise. In the absence of a target, the average internal state is well below threshold and noise-induced spikes recruit little additional activity. In contrast, the presence of a target pushes the average internal state closer to threshold, such that each spike is now able to recruit additional spikes, leading to a chain reaction. Our results show that while individual BIDS nodes may be noisy and unreliable, a network of BIDS nodes is capable of highly reliable detection even when the signal-to-noise ratio (SNR) on individual nodes is low. We demonstrate that collective computation between nodes supports improved detection accuracy in a manner that is extremely robust to the damage or loss of individual nodes.

There is a need for cheap sensors that are robust, energy efficient, and easy to deploy as an ad-hoc network for monitoring large, remote, and often inaccessible areas. Such a network should be infinitely scalable and robust to failing sensors Estrin et. al. In a typical scenario, the sensor network could be tasked with notifying an outside observer of the presence and approximate location of a particular target. One such scenario is monitoring the security of national borders by detecting people and vehicles. Conventional distributed sensor network (DSN) nodes require abundant bandwidth to communicate to a global hub where information is integrated to make a detection decision. The bandwidth required for many nodes to communicate to a single central computer or cluster head can be prohibitive and have considerable power requirements Heinzelman et. al. Moreover, the expense of the hardware required to allow such communication protocols is not feasible for deployment over large, remote areas Estrin et. al.. Here we propose a solution to such a remote monitoring problem with Biologically Inspired Distributed Sensors (BIDS). BIDS nodes are designed to be inexpensive enough to be distributed liberally across a wide area and able to be configured into a topology that is resilient to changes, such as nodes failing or new nodes entering. Because BIDS nodes use biologically motivated spike-based communication and simple analog components, they would consume significantly less power than a conventional DSN node that uses digital communication protocols. Thus, BIDS nodes could potentially be powered by scavenged resources, such as solar energy collected from cheap photovoltaic panels.

Individual BIDS sensors are expected to have a low signal-to-noise ratio (SNR) because of the need to make them small, cheap, and low power, which allows deployment over large, remote regions. However, the dynamics of the BIDS network can greatly improve the SNR through collective computation. To enable these dynamics, the individual BIDS node is modeled after a leaky-integrate-and-fire (LIF) neuron (Worgotter et. al., Burkitt et. al.). Input to the sensor head increases the internal state potential of a node, which decays with a characteristic time constant. The inputs to the node are summed in parallel, which pushes the node toward a threshold. When the threshold is reached, the node fires an omnidirectional spike (mimicking an action potential in a neuron) represented by a radio-frequency (RF) pulse. The nodes within a given radius detect the spike, which increases their internal state potential. This form of communication is inspired by the concept of synaptic interactions between biological neurons. Whereas synaptic interactions between biological neurons can be either excitatory or inhibitory, depending on whether the resulting input drives the internal state closer or further away from threshold, interactions between BIDS nodes are purely excitatory and thus always act to produce an overall increase in excitation across the network. When one BIDS node fires a spike, it brings all the other BIDS nodes within its communication radius closer to their firing thresholds. If no target is present, and thus the individual sensor heads are responding solely to environmental noise, most nodes will be far from threshold and the net effect of any single spike will be negligible (i.e. unlikely to elicit any additional spikes). On the other hand, if a target is present, such that the individual sensor heads over some stimulated region are all receiving additional drive, then more of the nodes in the local network will be close to threshold and any single spike can elicit multiple additional spikes. Thus, the presence or absence of a target determines whether any single spike evokes a chain reaction, thereby causing a large general increase in firing activity across the stimulated portion of the network. In this way, even a relatively weak target signal can be amplified by collective interactions, producing a much more detectable signal with a much higher SNR. Here we will describe communication between BIDS nodes (RF pulses) as lateral interactions in order to distinguish them from the internal communication from the sensor head that is detecting the raw stimulus.

BIDS networks are scalable, robust and fault tolerant because each individual node performs local computations to determine if a detection has occurred. Because each BIDS node operates in a self-contained fashion, integrating its sensor input with generic input from surrounding nodes, the network can be extended or augmented simply by adding additional nodes. The individual nodes within a BIDS network do not need to communicate to a central computer, which greatly lowers bandwidth requirements and alleviates the need for complex communication protocols to deal with collisions and changing topologies due to damage, node loss, and other factors. The extremely simple and robust spike-based communication protocol also replaces the need for a MAC protocol Chaari. These latter two points also give evidence that the BIDS nodes will have lower power requirements. Additionally, it has been well established that neuromorphic hardware requires substantially less power than digital hardware.

The focus of this work is to model a mechanism for a BIDS network to detect a signal reliably in spite of low signal-to-noise due to poor individual detectors and environment noise. This is accomplished through collective computation via interactions between the nodes. Although we will touch on proposed hardware and communication mechanisms, there are no fixed specifications for the individual nodes.

Others have demonstrated a capability for high coverage, low power, inexpensive, and robust DSNs. To achieve high coverage, Howard presented mobile nodes that repel other nodes and obstacles to spread coverage with minimal nodes. This facilitates a large network coverage by distributing the sensor nodes as sparsely as possible. Although BIDS nodes are currently designed to obviate the need for mobile nodes, the algorithm and architecture can be used in mobile nodes to reduce the overall operating power.

Power considerations were explored in Zhang et. al, which argues that a dense network of nodes is detrimental to the network in several ways. The problem was alleviated by choosing a subset of the network's nodes to stay on for low power usage. We argue that multiple nodes with cheap hardware can achieve better detections as a dense network than as a sparse network. A dense network also reduces power consumption for each individual node because of low transmission range requirements.

Other work has explored the insight biology gives into dynamic networking, self-calibration, and peer to peer communications by modeling ants (Theraulaz et. al.), cells (Hayes et. al.), fireflies Wokoma et. al., and bees Boonma. Specifically, Boonma et. al. presented modeling biological behaviors such as ``energy exchange, pheromone emission, replication, migration and death'' to achieve autonomous, scalable, adaptable, self-healing, and simplistic networks. Similarly, we propose a DSN that models a network of neurons to achieve detections by exploiting a local, embedded algorithm.

Other research done by Estrin et. al. and Intanagonwiwat et. al. is closely related to what BIDS is trying to accomplish. Estrin et. al. looks into local algorithms and a data-centric network. It is argued that each node does not need an identity to be useful to the network. This allows for a robust network that is resilient to topological changes. Directed diffusion Intanagonwiwat et. al., although local and robust, requires a query-driven data delivery model, as in Akkaya et al. We are looking at a specific scenario of monitoring a large, inaccessible area where a simple detection scheme suffices. We believe that a sensor network with a simple communication protocol will lead to accurate detections for lower costs than Intanagonwiwat et. al. predicts.

There is a large demand for a universal algorithm to process and understand written language. This technology would be useful to improve web search results, news parsing and summarizing, internet monitoring for national security, and in the case of Yelp - analysis of user-submitted reviews. Here I propose the use of methods developed for video processing of natural scenes to exploit the underlying deep structure of language for text analysis. The method of learning deep structure from data has been highly successful in image and video analysis, but not explored thoroughly for text analysis. The algorithm outlined below, hereforth called the Locally Competitive Algorithm (LCA), draws from principles in neural processing as well as deep sparse generative models. The LCA model solves an overcomplete “L0” sparse approximation problem to construct a basis set of low-level features that can be combined to represent written language. Although the overall solution is similar to that produced by other sparse approximation algorithms, such as matching pursuit, the method for finding the L0 minimum incorporates neural dynamics that will exploit the spatial and temporal structure found in language. The LCA model is also highly amenable to a hierarchical configuration, which is necessary for understanding deep structure in language. The proposed hypothesis is that a deep sparse generative model implemented using LCA dynamics will learn structure from text language to allow for inference of defining characteristics such as “helpfulness” using the labeled data provided for the Yelp Dataset Challenge.

Like the visual world, written language is highly structured and very diverse. Here I propose a method for exploiting such structure using algorithms originally designed to emulate neural processing in the visual cortex [8]. The objective is to represent text input in such a way that a manifold can be learned from unsupervised presentation of the data. The method for learning the manifold is derived from state-of-the-art algorithms in the field of image and video understanding. The proposed model will represent the learned manifold as a deep, overcomplete basis set. This basis set can be learned in an unsupervised fashion with a large corpus of exemplary text data. The model will form a representation for it’s input in terms of the learned basis set using sparse approximation methods. Using this representation, the model will be capable of “reconstructing” it’s input through a feedback driven, generative pathway while activating as few of the basis elements from the set as possible. The sparse representation can also be used as a descriptor for a given input. This descriptor can be paired with labeled text data, such as the “helpfulness” rating associated with reviews in the Yelp Dataset, to teach a classifier to predict labels from unlabeled data.

To accomplish the objective described above, I will use methods traditionally used for finding structure in the visual world. One such method, called sparse approximations of overcomplete basis sets, has been used extensively in image and video analysis research (most of which are extensions of). Deep, generative models are also among the state-of-the-art techniques for understanding visual data. It is expected that these methods strongly parallel algorithms found in neural processing. Recently, the Locally Competitive Algorithm (LCA) was proposed as a possible method for solving the sparse approximation problem. The algorithm exploits neural dynamics to find a minimum to the L0 sparse approximation problem that is closer to the absolute minimum than traditional, greedy sparse approximation methods, such as Matching Pursuit.

In 2008, Rozell et al. introduced locally competitive algorithms (LCAs) to solve the problem of sparsely encoding images using an overcomplete basis set (or to borrow a fitting term from computer vision literature - dictionary). For example, if x is an image (Fig 1, Top Left), then x can be pieced together as a superposition of image patches (Fig 1, Bottom), so that x=ak where the k are the dictionary elements and ak are scalar coefficients. Depending on the number of dictionary elements used, the reconstructed image can closely approximate the original (Fig 1, Top Right). The original algorithm utilized a dictionary which was constructed offline via an alternative method and specifically focused on sparse approximation for reconstruction.

For an overcomplete dictionary, any image can be represented in infinitely many ways. This dictionary will represent the underlying structure, or manifold, that the input data lies on. Each new input can then be represented using a small number of dictionary elements using a method called sparse encoding. The basic form of the sparse encoding problem is to find a representation of x using the minimum number of nonzero coefficients. Formally, we desire to solve a least-squares optimization problem with an L0-type sparseness penalty, which is equivalent to minimizing the following energy function E.

Where H() is the Heaviside function, H(y >0) = 1, H(y0) = 0. However, this optimization problem is NP-hard. Accordingly, approximate-solution techniques have been developed. These solutions include Matching Pursuit (MP) for attempting to solve the L0 penalty, or casting the problem as an L1 penalty and solving for a similar minimum, although not identical. Typically, the L1 penalty is solved using Basis Pursuit Denoising (BPDN).

In contrast to either BPDN or MP, the LCA represents a robust, extensible, and intrinsically parallel method for minimizing energy functions of the following general form.

Where C is a sparseness penalty. This general equation is ubiquitous in literature on deep, sparse generative models for image and video understanding. However, it has yet to be applied to understanding structure in language.

At Los Alamos National Laboratory, we have extended the LCA model to allow for learning of a sparse, overcomplete basis set. We have also furthered the model by implementing it in a deep, hierarchical, generative network. The model is implemented in PetaVision [7], a high-performance neural simulation toolkit. Our LCA implementation has allowed us to exploit inherent massive parallelism to analyze data on state-of-the-art supercomputers. We have also demonstrated the ability to learn a hierarchy of dictionaries to represent deeper structure of the input. An analogy to “deep structure” can easily be made for language processing: the lower-level dictionary will represent letter and word (n-gram) combinations, while the higher-level dictionary will represent concepts and ideas derived from sentences or paragraphs. Our LCA model can continuously learn as novel data is received, or a learned dictionary can be fixed to improve algorithm run-time.

I have further extended the LCA model to allow for text input to be formatted and processed. The text is represented as a large set of dictionary elements that can be combined with a weighted sum to form the original input. This process is unsupervised and only requires an ASCII text file input. The algorithm is assembled as a two-layer hierarchy for deep representations with feedback to allow for reconstructing inputs. I have assembled a large, internet-sourced database of peer-reviewed and edited text documents to allow the model to effectively learn language structure without the noise associated with colloquial english (e.g. slang). Using the open-source data as an example of proof-of-concept, I have filed a preliminary patent on using hierarchical LCA networks for language understanding.

A fully-trained LCA model could be used for a variety of purposes by the Yelp community. The most immediate application would be to use the sparse approximation vector (i.e. the list of ak components) generated for reconstruction as a label vector to determine a degree of usefulness, funniness, or coolness for a rating. Given an input, the LCA model generates a list of coefficients, or weights, associated with each of it’s dictionary elements. This sparse vector is then used in a weighted summing process to reconstruct the input. However, the sparse vector could also be associated with a label (e.g. “helpfulness”) to a traditional machine learning algorithm, such as a Support Vector Machine. The trained SVM could then produce a degree of helpfulness for a novel, unlabeled review. This principle could be applied to a variety of other label types. Another example would be using the rating text to generate a predicted star rating for a restaurant.

The ability for the LCA model to reconstruct it’s input also could be exploited to generate a brief summary of a restaurant by forcing the model to reconstruct a restricted descriptor (say, 100 words maximum) from a large set of reviews for a single restaurant. This may be useful for search purposes or as an alternative to associative key-words.

Viewpoint invariant object detection in natural images has been a long-standing challenge in computer vision. To date, algorithms exist that have demonstrated 80–90% performance with single category recognition tasks and 10 to 100 training images. However, these algorithms are meant to simulate human performance by quickly recognizing tens of thousands of categories with consistently high accuracy. Li Fei-Fei's recent work from Stanford University has shown that performance of common algorithms decreases to as low as 5% with 10,000 categories. This decrease in performance can be attributed to the limited amount of training stimulus presented to the neural networks. It is estimated that the average person receives roughly one TeraPixel/day (10^12 pixels/day) of visual information. To properly represent the human visual experience, an algorithm would have to be trained with over 2-million average-sized images per day, which is impossible given current image datasets. It is therefore interesting to predict how performance for current algorithms scales with number of training images. Here I explore the performance of a Los Alamos National Laboratory viewpoint-invariant, hierarchical model of the visual cortex using various amounts of training data from the Stanford ImageNet database, one of the most well populated image datasets available. I will compare the single-object detection results with theoretical performance estimations for increasing amounts of training data. This data could then be used to predict how well PANN will perform with many-category recognition tasks.

Anatomical and physiological data suggests that the ventral visual pathway of the primate brain is subdivided into specialized processing modalities. Here we combine a model of color/texture processing with a separately developed model of shape/form processing to determine whether such pathways can be both functionally independent and complimentary. Our hypothesis was that the combination would yield better performance on an invariant object localization and classification task than either model alone. Functional independence is established if the optimal combination corresponds to the Boolean rules used for combining two statistically independent binary classifiers. To extract color/texture information, we learned a sparse dictionary of features from representative training data, pooled the dictionary elements using a winner-take-all heuristic and then clustered the pooled data using a k-means algorithm. In tandem, we extracted shape information by preprocessing the image with a canny edge filter, then computed difference kernels based on co-occurrence statistics for edge combinations characteristic of the target category. We represented the two pathways as binary classifiers and combined them with optimal Boolean operators, as defined using a Neyman-Pearson theorem for Receiver Operating Characteristics (ROC) curves. Using ground-truth for a high-definition video-stream from a helicopter flying over Los Angeles, CA, we demonstrate that the two pathways are functionally independent and when combined perform substantially better than either pathway alone. Our results suggest that the separate processing modalities found in the primate ventral visual pathway represent functionally independent and complimentary approaches to viewpoint invariant object detection and localization.

Studies have found separate neural populations for orientation and color selectivity in the primary visual cortex (V1) of Macaque and Human subjects. These V1 domains have been correlated with functionally specialized pathways in V2 \cite{AR:1}, which play a role in integrating separate visual functions. Specifically, in V2, thin stripes, interstripes (pale stripes) and thick stripes contribute to perceptual attributes of color, static form (contours) and motion/depth/dynamic form (surfaces), respectively \cite{SS:1}. The functional separation of visual processing modalities is also pronounced in V4 with distinct feature representations. Evidence suggests that the integration of these processing streams either occurs in V4 or at the next level, IT, where representations of complex objects are found. In this study, we investigate the hypothesis that decomposing an input scene into separate processing modalities, consistent with key aspects of the functional separation seen in the ventral visual stream, improves object segmentation and identification relative to that achieved by either pathway independently.

We present two neurally inspired models of cortical visual processing: one representing color/texture and the other representing shape (or static form), corresponding to two of the modalities identified in the ventral stream. Our results show that these two models are functionally independent and thus may maximally complement each other when combined. We define a pair of binary classifiers to be functionally independent when the Boolean operators used to combine them are those that are deemed optimal for statistically independent classifiers in the Neyman-Pearson sense: for a given upper bound on false alarms (false positives), the combination rules maximize the detection rate (true positives) \cite{MB:1}. We demonstrate that combining color/texture- and shape-processing streams results in qualitatively and quantitatively improved results on a viewpoint-invariant object detection and localization task. This work has been introduced previously without demonstrating functional independence and with an ad hoc combination procedure. Performance is measured using a dataset that is orders of magnitude larger than several popular benchmarks, such as CalTech256, thereby helping to alleviate statistical biases arising from finite sample size.

The primate visual cortex has provided inspiration for many state-of-the-art computer vision models. Performance comparisons with human subjects as well as with non-biological computer vision models are made regularly on a variety of benchmark datasets. The vision tasks emphasized by the datasets vary from labeling whole scenes to localizing objects and testing invariance to identity-preserving transformations including translation, scaling, rotation, as well as invariance to noise and illumination.

The HMAX model, which was explicitly designed to emulate visual processing in the primate ventral stream, along with several extensions to the HMAX framework, all utilize spatial pooling over vector representations. At the first processing level, pooling produces representations similar to those found in V1 complex cells, which have some tolerance to object-preserving translations. HMAX models perform well at whole-scene categorization tasks and are thought to utilize both shape-based and texture-based features. However, HMAX models do not naturally allow the relative contributions of color/texture vs. shape pathways to be assessed independently. Our color/texture algorithm can be viewed as an extension of the HMAX model, where we explicitly emphasize the color and texture information at the cost of losing local shape information due to spatial pooling.

SIFT descriptors utilize hand-designed features that are optimized to represent objects in natural scenes, yielding state-of-the-art performance on datasets testing for invariance to viewpoint, blur, illumination, rotation, and scale. However, SIFT descriptors do not explicitly account for shape features and have recently been shown to perform less well at invariant object recognition tasks when compared to more biologically inspired models. Cellular neural-network models, generative models, and visual saliency models all have a neuromimetic basis, but make no explicit attempt to divide processing into distinct color/texture vs. shape modalities.

A number of image segmentation algorithms rely on color (RGB) values in images to differentiate objects in the scene, but such approaches do not provide specific object labels. Farabet et al. combine RGB-based segmentation with a deep, multi-scale convolutional neural network to detect and localize a large number of distinct object classes. However, like all HMAX-like algorithms, the use of pooling to achieve viewpoint invariance removes much of the spatial relationships needed to accurately represent object-specific shape features.

There exists several shape processing models that utilize either a Histogram of Gradient (HOG) descriptors, textons or geons. These algorithms differ greatly from our shape-processing algorithm in that we learn a general descriptor of edge co-occurrence for a given target object, instead of a set of shape primitives (e.g. geons, textons) or a set of edge orientations (e.g. HOG).

The concept of combining multiple classifiers for computer vision applications is not new. Varma and Ray explore the advantages of combining classifiers for achieving greater invariance to various identity-preserving transformations, although they do not consider independence or the domain of features that the classifier utilizes. Additionally, Bosch et al. successfully combined algorithms based on SIFT features for color/texture processing and Pyramid of Histograms of Orientation Gradients (PHOG) features for shape processing. They evaluated both the independent and combined results on the CalTech101 dataset and showed improved performance with the combined results. The PHOG algorithm generates a histogram of edge orientation occurrences across three tiers of a spatial pyramid, accounting for local shape and varying degrees of spatial correspondence. However, it has yet to be tested if PHOG features represent object shape in a dataset where shape information is encoded differently from edge statistics. The SIFT and PHOG features are combined using a weighted summation. The summed kernel is classified with a Support Vector Machine (SVM), which confounds the separate contributions and implements shallow classification architecture. In contrast, our algorithm maintains separation of processing modalities all the way through to the final classification and is hierarchically organized into tiered processing stages representing higher cortical areas (V2, V4).

Our algorithms were analyzed qualitatively by observing the hits with respect to the ground-truth bounding boxes. We also evaluated an overall score for the combined output using scoring software provided by DARPA for the NeoVision2 grand challenge (section 5.3).

Here we demonstrate that functionally independent processing modes can be combined to increase performance at visual object identification and localization tasks. This work validates theories that distinct modalities exist in the primate ventral visual cortex, and that the separated pathways contribute to a better overall perception of our visual world.

The technology underlying ODD kernels was developed in PetaVision, a massively parallel neural simulation toolbox for conducting high-performance simulations of large networks. The model was motivated and guided by a psychophysical object detection task employing computer-generated images.

In each layer of processing, we trained two kernels that represented the target class (cars) and the distractor class (the rest of the scene). The kernels were trained using ground truth bounding boxes, where edge detectors with a receptive field within the boxes are maximally supported for the target class, and vice versa for the distractor class. The neurons were supported via pair-wise interactions, although this could theoretically be expanded to n-wise interactions for improved performance. The final stage of processing for each layer is the ODD kernel, which represents the normalized difference between the target and distractor kernels. We trained 5 layers in this fashion, with layer afferents coming directly from previous layers (the first layer received the canny-filtered image as an input). The number of activated neurons was reduced by nearly 4 orders of magnitude from about 88,000 in the first layer, prior to any lateral interactions, to 214 in the final layer (figure 2). This was all done in real time on a 16-node, 8 core-per-node computer.

For the DARPA NeoVision2 challenge, we modified our high-performance computer vision model (PANN) to learn a sparsified over-complete color/texture feature dictionary for the dataset (figure 3-left). Our retinal model down-samples the input frame to remove video compression artifacts, reduce computational expense, and de-emphasize some of the high-frequency luminance information. Our primary visual cortex (V1) S-cell layer uses greedy matching pursuit to build a local sparse dictionary representation. The dictionary elements are learned using a Hebbian rule. Our S-cell columns are very sparse, typically with less than 5% of local feature detectors active in any given column. However, they still allow for good reconstruction of the input image in distinct to standard HMAX approaches (figure 3-right). Our V1 C-cell layer applies a local max pooling operation, producing a translation-tolerant representation of image patches. This pooling operation will cause a loss of any local shape information, but will not significantly affect the ability to identify objects using color and texture information. Even after pooling, the C-cell columns are quite sparse. Note that this processor does not use frame differencing. This allows us to detect objects that are stationary within the frame.

For object detection and classification, we used a multi-category generative model based on k-means clustering of the sparse C-cell column responses. We trained this model in a semi-supervised way, allowing the image background to divide up into 30 unlabeled categories (setting this number is a meta-learning task). The categories, on inspection, appear strongly correlated with naturally occurring background scene object categories, including tree foliage, grass, pavement, water and beach (figure 4). We then augmented this set of background categories with the target ‘car’ category learned using the same sparsifying dictionaries for labeled image patches (i.e. we used supervised learning for target categories, only). The final image patch classifier algorithm is a Euclidean (L2) minimum distance classifier in this multi-category space of category mean vectors. The multi-category classifier is a small component of the overall computation time (which is dominated by the formation of sparse representations in V1 S-cell columns) and produces whole scene visualizations that provide additional contextual cues (e.g. correlation of cars with roads or boats with water) that could be exploited for object detection, but are not utilized here.

Each classifier was represented as a binary detector. For the shape-processing model, we only used the final kernel level and counted all remaining edges as ‘car’ hits. For the color/texture model we counted all pixels that were closer to the ‘car’ cluster than any other cluster as hits. The hit rate and false alarm values were derived on a pixel-level, where all pixels inside a ground-truth box were considered ‘car’ and all pixels outside were considered ‘not car’. This necessitated radial dilation of the classifier hits to fill in more of the box area. We populated the ROC curve by steadily increasing the dilation radius and observing the resulting hit and false alarm values.

First we chose a false-alarm threshold for the two input classifiers. Then we computed the optimal Boolean combination rules (5 out of a possible 16) following the procedure outlined in to create what they term the ‘LR-ROC’ curve. This curve gives ROC data points that represent the ideal set of Boolean algebra combinations of the input binary classifiers. Because our algorithms were functionally independent, the optimal set was the same as what would be used for a convex hull ROC analysis. Ignoring the two trivial points (0,0) and (1,1), we can maximally reduce the false alarm rate by performing an ‘AND’ operation on the algorithms, and maximally increase the hit rate by performing an ‘OR’ operation. The medial point between the two operations is the classifier with the highest likelihood ratio, which is the color/texture processor for the false-alarm threshold we chose.

The individual classifier outputs were represented as logical binary images, with a value of 1 indicating ‘car’ hits. These images were combined using the optimal rule for the given false-alarm rate, described above. The matrix of hits was then clustered using a spatial clustering algorithm called DBSCAN. The algorithm iterates through all points and clusters them based on a density metric that is computed from two input parameters. We optimized the parameters based on the ROC performance of the clusters. Once the hits were clustered, we found the minimum volume enclosing ellipsoid to get the appropriate size and orientation of the bounding box. The final output was a CSV file of bounding-box locations, labels, and label confidences, in a format defined by DARPA for the NeoVision2 grand challenge (see section 5.3).

The dataset used for this test is composed of image ``chips'' (cropped portions) from the DARPA NeoVision2 Heli dataset. The task to be solved was to distinguish, via whole image classification, between target chips containing cars and  distractor chips not containing cars. For training, 24,589 chips of cars and distractors (12,293 of each) were extracted. For testing, 1,186 chips were extracted from a completely disjunct video clip in the same dataset. The car chips were cropped according to bounding boxes given by ground truth data. The same number of equivalently sized distractor chips were cropped from portions of the video that did not overlap with car bounding boxes. We also excluded Don't Care Regions (DCRs) from the distractor chips. These regions were specified by the ground-truth files and often included dense groups of cars.

We computed Color SIFT features using the VLFeat vl_phow function. Dense, Color (HSV) SIFT features were computed using a grid step size of 10 and bin sizes (scales) of 4, 8, 12 and 16 for increased scale invariance. All other PHOW parameters were left to default values. Visual words were computed from the SIFT data using the vl_kmeans function with 31 words and ELKAN optimization algorithm specified. The words were then quantized using the vl_kdtreebuild function with default parameters. A Vector of image descriptors was computed by histogramming the resulting kdtree bins. The histograms are converted into a kernel using the vl_homkermap function with a gamma value of 1. Finally, vl_svmpegasos is called to implement a PEGASOS linear SVM solver.

PHOG feature histograms were computed using the MATLAB code provided. The histograms were converted into a kernel using the same parameters for vl_homkermap as described above. This kernel was used to train a SVM classifier with parameters identical to the one used for the SIFT features.













