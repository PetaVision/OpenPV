//
// MLPTest.params
//
// created by slundquist: Mar 21, 2014
//

//A params file to test PV implementation of a multilayer perceptron
//This test is solving the xor problem
//http://www.mind.ilstu.edu/curriculum/artificial_neural_net/xor_problem_and_solution.php?modGUI=239&compGUI=1286&itemGUI=2253

debugParsing = true;    // Debug the reading of this parameter file.

HyPerCol "column" = {
   nx = 16;  
   ny = 16;
   dt = 1.0;  //time step in ms.	     
   randomSeed = 1234567890;  // Must be at least 8 digits long.  // if not set here,  clock time is used to generate seed
   startTime = 0.0;
   stopTime = 200; //180000001; //200000;
   progressInterval = 100.0; //Program will output its progress at each progressStep
   writeProgressToErr = false;  
   //outputPath = "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/";
   filenamesContainLayerNames = true;  
   filenamesContainConnectionNames = true;
   checkpointRead = false;  
   checkpointWrite = false;
   checkpointWriteDir = "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/Checkpoints/";
   checkpointWriteStepInterval = 10000;
   deleteOlderCheckpoints = false;
   outputNamesOfLayersAndConns = "LayerAndConnNames.txt";
   suppressLastOutput = false;
};

ParameterSweep "column":outputPath = {
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/0/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/1/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/2/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/3/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/4/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/5/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/6/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/7/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/8/";
   "/nh/compneuro/Data/MLPLCA/LCA/cifar_classvis/9/";
};

//
// layers
//

ConstantLayer "onesLayer" = {
    restart = 0;
    nxScale = 1; 
    nyScale = 1;
    nf = 1;
    writeStep = -1;
    initialWriteTime = 0.0;
    mirrorBCflag = 0;
    writeSparseActivity = false;
    InitVType = "ConstantV";
    valueV    = 1;
    VThresh = -infinity;   
    phase = 0;
};

////The input layer of data

LeakyIntegrator "Input" = {
    restart = 0;
    nxScale = 1;
    nyScale = 1;
    nf = 48;
    writeStep = -1; //Change based on display period
    initialWriteTime = 0.0; //Change based on display period 
    mirrorBCflag = 0;
    writeSparseActivity = 0;
    InitVType = "ZeroV";
    VThresh = -infinity;
    dropoutChance = 0;
    integrationTime = 1;
    phase = 2;
};

//ANN layer to take the input and put any negative numbers to 0
ANNLayer "deNormalize" = {
    restart = 0;
    nxScale = 1;
    nyScale = 1;
    nf = 48;
    writeStep = -1; //Change based on display period
    initialWriteTime = 0.0; //Change based on display period 
    mirrorBCflag = 0;
    writeSparseActivity = 0;
    InitVType = "ZeroV";
    VThresh = 0; //Set to 0
    dropoutChance = 0;
    phase = 3;
};

//Recon layer for projection
ANNLayer "Recon" = {
    restart = 0;
    nxScale = 2;
    nyScale = 2;
    nf = 3;
    writeStep = 100.0;
    initialWriteTime = 100.0; //1 + writestep
    mirrorBCflag = 0;
    writeSparseActivity = 0;
    //
    InitVType = "ZeroV";
    //
    VThresh = -infinity;
    VMax = infinity;     // prevent reconstruction from exceeding reasonable bounds
    VMin = -infinity; 
    VShift = 0;
    VWidth = 0;
    valueBC = 0;
    phase = 4;
};

MLPForwardLayer "ForwardLayer1" = {
    restart = 0;
    nxScale = .0625;
    nyScale = .0625;
    nf = 1000;
    writeStep = -1; //Change based on display period
    initialWriteTime = 0.0; //Change based on display period 
    mirrorBCflag = 0;
    writeSparseActivity = 0;
    InitVType = "ZeroV";
    VThresh = -infinity;
    dropoutChance = 0;
    phase = 3;
};

MLPSigmoidLayer "HiddenLayer1" = {  // 2
    nxScale           = .0625;
    nyScale           = .0625;
    nf                = 1000;
    originalLayerName = "ForwardLayer1";
    InitVType         = "ZeroV";
    valueV            = 0;
    mirrorBCflag      = 0.0; 
    restart           = 0.0;      // from graylast
    linAlpha            = 0.1;
    spikingFlag       = 0.0;
    writeStep         = -1;
    writeNonspikingActivity = false;
    phase = 4;
};

MLPForwardLayer "ForwardLayer2" = {
   #include "ForwardLayer1";
   @nxScale = .0625;
   @nyScale = .0625;
   @nf      = 250;
   @phase = 5;
};

MLPSigmoidLayer "HiddenLayer2" = {  // 2
    #include "HiddenLayer1";
    @nxScale           = .0625;
    @nyScale           = .0625;
    @nf                = 250;
    @originalLayerName = "ForwardLayer2";
    @phase = 6;
};

MLPForwardLayer "ForwardLayer3" = {
   #include "ForwardLayer1";
   @nxScale = .0625;
   @nyScale = .0625;
   @nf      = 75;
   @phase = 7;
};

MLPSigmoidLayer "HiddenLayer3" = {  // 2
    #include "HiddenLayer1";
    @nxScale           = .0625;
    @nyScale           = .0625;
    @nf                = 75;
    @originalLayerName = "ForwardLayer3";
    @phase = 8;
};

MLPForwardLayer "ForwardLayerFinal" = {
   #include "ForwardLayer1";
   @nxScale = .0625;
   @nyScale = .0625;
   @nf      = 10; //For 10 categories
   @phase = 9;
};

MLPOutputLayer "OutputLayer" = {  // 2
    nxScale           = .0625; //Doing 2 by 2 since this will test multiprocess as well
    nyScale           = .0625;
    nf                = 10;
    originalLayerName = "ForwardLayerFinal";
    InitVType         = "ZeroV";
    valueV            = 0;
    mirrorBCflag      = 0.0; 
    restart           = 0.0;      // from graylast
    linAlpha            = 0.1;
    spikingFlag       = 0.0;
    writeStep         = -1;
    writeNonspikingActivity = false;
    //Local target in training will be different than local target in testing: this will test that each x/y network gets the same answer
    localTarget = false; //Reducing across all output layers
    statProgressPeriod = 1;
    gtLayername = "GroundTruth";
    phase = 10;
};

ConstGTLayer "GroundTruth" = {
    restart = 0;
    nxScale = .0625; 
    nyScale = .0625;
    nf = 10;
    writeStep = -1;
    initialWriteTime = 0.0;
    mirrorBCflag = 0;
    writeSparseActivity = false;
    InitVType = "ZeroV";
    //define a linear relation between its input and output, with some hard cut-off.
    VThresh = -infinity;   
    //gtVal = 1; //Automobile
    phase = 9;
};

//Parameter sweep of GroundTruth gtVal
ParameterSweep "GroundTruth":gtVal= {
    0; 1; 2; 3; 4; 5; 6; 7; 8; 9;
};

MLPErrorLayer "ErrorFinal" = {
    restart = 0;
    nxScale = .0625;
    nyScale = .0625;
    nf = 10;
    writeStep = -1; //Change based on display period
    mirrorBCflag = 0;
    writeSparseActivity = 0;
    linAlpha            = 0.1;
    ForwardLayername = "ForwardLayerFinal";
    InitVType = "ZeroV";
    VThresh = -infinity;
    phase = 11;
};

MLPErrorLayer "Error3" = {
   #include "ErrorFinal";
   @nxScale = .0625;
   @nyScale = .0625;
   @nf = 75;
   @ForwardLayername = "ForwardLayer3";
   @phase = 12;
};

MLPErrorLayer "Error2" = {
   #include "ErrorFinal";
   @nxScale = .0625;
   @nyScale = .0625;
   @nf = 250;
   @ForwardLayername = "ForwardLayer2";
   @phase = 13;
};

MLPErrorLayer "Error1" = {
   #include "ErrorFinal";
   @nxScale = .0625;
   @nyScale = .0625;
   @nf = 1000;
   @ForwardLayername = "ForwardLayer1";
   @phase = 14;
};

//Connections
IdentConn "InputTodeNormalize" = {
    preLayerName = "Input";
    postLayerName = "deNormalize";
    channelCode = 0; //Excitatory Channel
    writeStep = -1;    
    delay = 0;
};

KernelConn "deNormalizeToRecon" = {
    preLayerName                        = "deNormalize";
    postLayerName                       = "Recon";
    channelCode                         = 0;
    weightInitType                      = "FileWeight";
    initWeightsFile                     = "/nh/compneuro/Data/CIFAR/LCA/data_batch_all15/Checkpoint20000000/S1ToError_W.pvp";
    useListOfArborFiles                 = false;
    combineWeightFiles                  = false;
    numAxonalArbors                     = 1;
    plasticityFlag                      = false;
    triggerFlag                         = false;
    pvpatchAccumulateType               = "convolve";
    preActivityIsNotRate                = false;
    writeStep                           = -1;
    writeCompressedCheckpoints          = false;
    selfFlag                            = false;
    combine_dW_with_W_flag              = false;
    delay                               = 0.000000;
    nxp                                 = 18;
    nyp                                 = 18;
    nxpShrunken                         = 16;
    nypShrunken                         = 16;
    nfp                                 = 3;
    shrinkPatches                       = false;
    updateGSynFromPostPerspective       = false;
    normalizeMethod                     = "normalizeL2";
    strength                            = 1;
    rMinX                               = 0;
    rMinY                               = 0;
    normalize_cutoff                    = 0;
    symmetrizeWeights                   = false;
    normalizeFromPostPerspective        = false;
    normalizeArborsIndividually         = false;
    minL2NormTolerated                  = 0;
    dWMax                               = 0.1;
    keepKernelsSynchronized             = true;
};

KernelConn "W1S1" = {
    preLayerName = "Input";
    postLayerName = "ForwardLayer1";
    channelCode = 0; //Prev layer to next err is on inhib b
    nxp = 1; 
    nyp = 1; 
    nfp = 1000;
    numAxonalArbors = 1;
    initFromLastFlag = 0;  // 1;  // restart
    writeStep = -1;
    
    weightInitType = "FileWeight";
    initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/W1S1_W.pvp";
        
    strength = 1.0;  
    normalizeMethod = "none";
    
    shrinkPatches = false;
    //writeCompressedWeights = 0.0;
    writeCompressedCheckpoints = false;
    plasticityFlag = 0;
    weightUpdatePeriod = 1.0;
    initialWeightUpdateTime = 1.0;
    dWMax = .00111; //sqrt(12288)/100000
    delay = 0;
     
    preActivityIsNotRate = false;
    selfFlag = false;
    shmget_flag = false;

    updateGSynFromPostPerspective = false;
    pvpatchAccumulateType = "convolve";
};

KernelConn "B1S1" = {
    preLayerName = "onesLayer";
    postLayerName = "ForwardLayer1";
    channelCode = 0; //Prev layer to next err is on inhib b
    nxp = 1; 
    nyp = 1; 
    nfp = 1000;
    numAxonalArbors = 1;
    initFromLastFlag = 0;  // 1;  // restart
    writeStep = -1;
    
    weightInitType = "FileWeight";
    initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/B1S1_W.pvp";
        
    strength = 1.0;  
    normalizeMethod = "none";
    
    shrinkPatches = false;
    //writeCompressedWeights = 0.0;
    writeCompressedCheckpoints = false;
    plasticityFlag = 0;
    weightUpdatePeriod = 1.0;
    initialWeightUpdateTime = 1.0;
    dWMax = .00111; //sqrt(12288)/100000
    delay = 0;
     
    preActivityIsNotRate = false;
    selfFlag = false;
    shmget_flag = false;

    updateGSynFromPostPerspective = false;
    pvpatchAccumulateType = "convolve";
};

KernelConn "W2" = {
   #include "W1S1";
   @nxp = 1;
   @nyp = 1;
   @nfp = 250;
   @preLayerName = "HiddenLayer1";
   @postLayerName = "ForwardLayer2";
   @initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/W2_W.pvp";
};

TransposeConn "W2T" = {
    preLayerName = "Error2";
    postLayerName = "Error1";
    channelCode = 0; //On excitatory channel
    originalConnName = "W2";
    selfFlag = false;
    preActivityIsNotRate = false;  // should help make response more indepenent of time step size dt
    writeStep = -1;
    writeCompressedCheckpoints = false;
    shmget_flag = false;
    delay = 0;
    pvpatchAccumulateType = "convolve";
    updateGSynFromPostPerspective = false;
};

KernelConn "B2" = {
   #include "B1S1";
   @nxp = 1;
   @nyp = 1;
   @nfp = 250;
   @preLayerName = "onesLayer";
   @postLayerName = "ForwardLayer2";
   @initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/B2_W.pvp";
};

KernelConn "W3" = {
   #include "W1S1";
   @nxp = 1;
   @nyp = 1;
   @nfp = 75;
   @preLayerName = "HiddenLayer2";
   @postLayerName = "ForwardLayer3";
   @initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/W3_W.pvp";
};

TransposeConn "W3T" = {
    #include "W2T";
    @preLayerName = "Error3";
    @postLayerName = "Error2";
    @originalConnName = "W3";
};

KernelConn "B3" = {
   #include "B1S1";
   @nxp = 1;
   @nyp = 1;
   @nfp = 75;
   @preLayerName = "onesLayer";
   @postLayerName = "ForwardLayer3";
   @initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/B3_W.pvp";
};

KernelConn "W4" = {
   #include "W1S1";
   @nxp = 1;
   @nyp = 1;
   @nfp = 10;
   @preLayerName = "HiddenLayer3";
   @postLayerName = "ForwardLayerFinal";
   @initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/W4_W.pvp";
};

TransposeConn "W4T" = {
    #include "W2T";
    @preLayerName = "ErrorFinal";
    @postLayerName = "Error3";
    @originalConnName = "W4";
};

KernelConn "B4" = {
   #include "B1S1";
   @nxp = 1;
   @nyp = 1;
   @nfp = 10;
   @preLayerName = "onesLayer";
   @postLayerName = "ForwardLayerFinal";
   @initWeightsFile = "/nh/compneuro/Data/MLPLCA/LCA/cifar_training_nodropout_slower/Checkpoints/Checkpoint40000/B4_W.pvp";
};

IdentConn "GroundTruthToErrorFinal" = {
    preLayerName = "GroundTruth";
    postLayerName = "ErrorFinal";
    channelCode = 0; //Excitatory Channel
    writeStep = -1;    
    delay = 0;
};

IdentConn "OutputLayerToErrorFinal" = {
    preLayerName = "OutputLayer";
    postLayerName = "ErrorFinal";
    channelCode = 1; //Inhibitory Channel
    writeStep = -1;    
    delay = 0;
};

//Connection to put the error to the image
KernelConn "Error1ToInput" = {
    preLayerName = "Error1";
    postLayerName = "Input";
    channelCode = 0; //Prev layer to next err is on inhib b
    nxp = 16; 
    nyp = 16; 
    nfp = 48;
    numAxonalArbors = 1;
    initFromLastFlag = 0;  // 1;  // restart
    writeStep = -1;
    
    //TODO here
    weightInitType = "OneToOneWeights"; //"UniformWeight";
    weightInit = 1;//Training value
        
    strength = 1.0;  
    normalizeMethod = "none";
    
    shrinkPatches = false;
    //writeCompressedWeights = 0.0;
    writeCompressedCheckpoints = false;
    plasticityFlag = 0;
    weightUpdatePeriod = 1.0;
    initialWeightUpdateTime = 1.0;
    dWMax = .00111; //sqrt(12288)/100000
    delay = 0;
     
    preActivityIsNotRate = false;
    selfFlag = false;
    shmget_flag = false;

    updateGSynFromPostPerspective = false;
    pvpatchAccumulateType = "convolve";
};
