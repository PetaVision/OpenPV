Welcome to the cloud. Here's everything you need to know about how to get your favorite run on Amazon's AWS services.

******************************************
Introduction
******************************************
Amazon AWS is a suite of tools to provide a virtual computing environment for end users. Here are the services we'll be using.

EC2: Elastic Compute Cloud. Allows you to create a virtual instance that run on Amazon hardware.
EBS: Elastic Block Store. The virtual hard drive that EC2 requires to do file io.
Snapshots: A snapshot of an EBS is essentially a copy of that hard drive.
AMI: Amazon Machine Image. Snapshots can be converted to AMIs, which contain the operating system, tools, and in our case, the PetaVision toolbox.
IAM: Identity and Access Management. Amazon's user account control that allows for user accounts for one AWS account.
S3: Simple Storage Service. Long term shared storage. We don't have file io access to S3; everything must be downloaded/uploaded through https or AWS' CLI. Databases and checkpoints will be stored here.

There exists a PetaVision AMI that has everything set up for PetaVision on GPUs. Additionally, the following software is installed on the image:
   Octave
   Python
   ffmpeg
   emacs

******************************************
Overview
******************************************
Here's a quick overview of how our instance is set up.
   GPU spot instance
   10GB root EBS that contains the OS, PetaVision and all software.
   50 - 100GB personal EBS that will store sandboxes and output files.
   S3 for storing databases and checkpoints.

Services that cost money:
   GPU Spot Instance: market price, hasn't risen over $.10 an hour yet
   EBS usage: $.10 per GB per Month
   S3 usage: $.03 per GB per Month
   Internet to EBS/S3 transfer: Free
   EBS/S3 transfer to Internet: cents per GB
   S3 to EBS and EBS to S3 transfers in the same region: Free

We are currently using the Oregon region.


******************************************
First time initialization
******************************************
This section explains how to get a new user set up on the aws account to do runs.

Activate your account:
   Email pv_office@rfd.newmexicoconsortium.org or contact an account administrator to create and set up your account. You should get a user name and a temporary password starting up.

(Current administrators) Activate a new account: 
   Log into aws console.
   Under services, click IAM.
   On the left tab, click Users.
   Click Create New Users.
   Enter a new user name.
   Deselect Generate an access key for each user. Each user will create their own access keys to access s3. Click create.
   Find the user that was just created, select it, click actions, and select Manage Password.
   Select Assign an auto-generated password, and select Require user to create a new password at next sign-in. Click apply.
   Click Download Credentials and send the file to the new user.
   Select the user that was just created, select it, click actions, and select Add User to Group.
   Select Developer, and click Add to Groups.

Log in to your account:
   Go to https://petavision.signin.aws.amazon.com/console.
   Open the credentials file given to you by the administrator to find out your user name and password.
   Sign in with your new user name and password.

Create your ssh key pair:
   Each instance must have an ssh key for security. Here's how to generate your own.
   Open a terminal and enter the following commands
   cd ~/.ssh (Create it if it doesn't exist)
   ssh-keygen -t rsa
   Follow the instructions on the screen. Note the filename in which you saved the key, ex: username_aws.
   ssh-add ~/.ssh/username_aws
   pbcopy < ~/.ssh/username_aws.pub (Copys contents of username_aws.pub to your clipboard for macs)
   Go to the EC2 management page, and find the option Key Pairs under Network and Security in the left tab.
   Click Import Key Pair and paste the public key contents in. Click import.


******************************************
Starting an Instance
******************************************
An AWS Instance allows the user to create a virtual environment to do runs. We do spot instances to save money, and already have the PetaVision capabilities to restart a run if it does die. For more information on spot instances, go to http://aws.amazon.com/ec2/purchasing-options/spot-instances/.

Creating a Spot Instance:
   Go to the EC2 management page. Click AMI under Images in the left tab.
   Find the PetaVision_GPU AMI. Select it and click launch.
   Pick a GPU instance (g2.2xlarge). Click next.
   Check Request Spot Instances under Purchasing option
   Type in an appropriate maximum price. Click next.
   No additional storage is required. Make sure Delete on Termination is checked. Click next.
   No tags are needed. Click next.
   Check Select an existing security group. Select the default security group. Click Review and Launch.
   After reviewing, click launch.
   Select Choose an existing key pair, and select your key pair that you created earlier.
   Check the acknowledgement, and click Request Spot Instance.

Spot instances will take a second to fulfill. Looking under Instances in the left tab, you can see the status of your request.
Once your instance is started, name the instance something relevant.


******************************************
Working with a running instance
******************************************
To ssh into your running instance:
   Go to the EC2 management page. Click Instances under Instances in the left tab.
   Find your instance in the list and note the Public IP address of the instance.
   The command to ssh into your instance is:
   ssh ec2-user@<public_ip_address>

To copy data to your instance:
   scp sourceFile ec2-user@<public_ip_address>


******************************************
Creating an EBS persistent volume
******************************************
AWS Spot Instances can die at any time. We will need an EBS persistent volume to store any run output we have as well as any sandboxes being ran from.

Create your volume:
   Go to the EC2 management page. On the left, click Volumes under Elastic Block Store. Click Create Volume.
   Adjust the size of the volume. 50 to 100 GB should be more than enough, depending on your run.
   Click create.

In order to use this volume in a running instance, we must mount it to a running instance.
Mounting your new volume to a running instance:
   Go to the EC2 management page. On the left, click Volumes under Elastic Block Store.
   Select the volume you want to attach. The state of the volume should say available.
   Click actions, and select attach volume.
   Click on the Instance box and select your running instance.
   Make sure the device is set to /dev/sdf. This is the default value.

A new EBS volume is completely blank. That means we need to format the volume to our instance filesystem. Note that this needs to be done only once per new volume.
Formatting your new EBS volume:
   Connect to your running instance.
   Run the following command:
   sudo mkfs -t ext4 /dev/sdf


******************************************
Setting up PetaVision
******************************************
Because the instance was booted from a PetaVision AMI, all of petavision should already exist in the home directory. There are several things needed to be done for PetaVision setup. 

Accessing your mounted volume in a running instance.
Connect to your running instance.
Run the following command:
   ~/startup.sh
   This will do 2 things: mount your data to the directory ~/mountData and update PetaVision and PVSystemTests.

Setting up your sandbox in your persistent volume:
   cd to mountData.
   Check out any sandboxes that you are planning on using. Ex:
   svn co http://svn.code.sf.net/p/petavision/code/sandbox/DepthLCA DepthLCA
   cd to ~/workspace.
   Edit CMakeLists.txt using your favorite editor.
   At the end of the file, add the absolute path to your sandbox to the cmakelists. Ex:
   add_subdirectory(~/mountData/DepthLCA ~/mountData/DepthLCA)
   Note that the same directory is put twice into the command add_subdirectory.

Building PetaVision
   cd to ~/workspace.
   Run the following command:
   ccmake .

Most of these variables should already be set up, but here are the important ones:
   CMAKE_BUILD_TYPE: True
   CUDA_GPU: True
   CUDA_RELEASE: True
   CUDNN: True
   CUDNN_PATH: ~/cuDNN/cudnn-6.5-linux-R1
   Open_MP_THREADS: True

   Press c to configure. Press e to exit the print statements.
   Repeat until the g option appears and press g. 
   cd to your sandbox and run make -j 8


******************************************
S3
******************************************
S3 is long term storage for amazon's cloud. S3 objects (files, images) can only be accessed through http, but is much cheaper than ebs volumes.

Create your access key:
   Go to the IAM management page. On the left, click users.
   Select your account and click user actions.
   Select Manage Access Keys.
   Click create access key. Click Download Credentials and open the file. You will need the access key and secret access key.

A s3 bucket is the highest level of abstraction.
Create your bucket:
   Go to the S3 management page.
   See if your database is already there (such as kitti or neovision2heli datasets). If they are, you are done.
   On the top, click Create Bucket.
   Name your bucket something appropriate.
   Make sure your region is the same as your ec2 users. Transferring from region to region costs money.
   Click create.

There are several ways to upload data to S3 to an existing bucket.

EBS Volume to S3 (preferred):
   One time setup:
   SSH into your running instance.
   Enter the following command:
   aws configure
   Enter your access key and secret access key. The rest can be left blank.

   Getting your database on S3:
   Download your database.
   cd to the outer directory of your database.
   Enter the following command to put your database on S3
   aws s3 cp --recursive myDatabaseFolder s3://yourBin/myDatabaseFolder
   Note that you must put myDatabaseFolder on both the destination and source to keep the directory structure on s3.
   Optional: Make sure you have all of your database on s3. Run the same command as above except for sync instead of cp.
   Once the data is on s3, you can delete the local data off the EBS volume.

Local to S3:
   Go to the S3 management page.
   Click an existing bucket that you would like your dataset to be, or create one and enter the bucket.
   Click upload.
   Drag the folder you would like to upload to the window.
   Click start upload.

Once you have your data on S3, you can specify a list of urls to feed to PetaVision Movie layer.
As an example:
   s3://yourBin/myDatabaseFolder/img00.png
   s3://yourBin/myDatabaseFolder/img01.png
   s3://yourBin/myDatabaseFolder/img02.png
   ...


******************************************
PetaVision Parameter Changes
******************************************
Changes to parameter files:
   In your movie layers, make sure your list of filenames is a list of s3 urls.
   Change your output directory to somewhere in mountData.
   Future work will be done to get checkpoints on s3.
