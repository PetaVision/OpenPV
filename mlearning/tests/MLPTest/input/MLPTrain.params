//
// MLPTest.params
//
// created by slundquist: Mar 21, 2014
//

//A params file to test PV implementation of a multilayer perceptron
//This test is solving the xor problem
//http://www.mind.ilstu.edu/curriculum/artificial_neural_net/xor_problem_and_solution.php?modGUI=239&compGUI=1286&itemGUI=2253

debugParsing = false;    // Debug the reading of this parameter file.

HyPerCol "column" = {
   nx = 2;   //2 inputs
   ny = 2;
   dt = 1.0;  //time step in ms.	     
   randomSeed = 2394853940;  // Must be at least 8 digits long.  // if not set here,  clock time is used to generate seed
   startTime = 0.0;
   stopTime = 1000.0; //100 training steps  
   progressInterval = 10.0; //Program will output its progress at each progressStep
   writeProgressToErr = false;  
   outputPath = "output/";
   filenamesContainLayerNames = true;  
   filenamesContainConnectionNames = true;
   checkpointRead = false;  
   checkpointWrite = false;
   suppressLastOutput = false; //Save the last output as checkpoint.
};

//
// layers
//

ConstantLayer "onesLayer" = {
    restart = 0;
    nxScale = 1; 
    nyScale = 1;
    nf = 1;
    writeStep = -1;
    initialWriteTime = 0.0;
    mirrorBCflag = 0;
    writeSparseActivity = false;
    InitVType = "ConstantV";
    valueV    = 1;
    VThresh = -infinity;   
    phase = 0;
};

//The input layer of data
//Note that the layer name matches train.txt
InputLayer "train" = {
    restart = 0;
    nxScale = 1; 
    nyScale = 1;
    nf = 2;
    writeStep = -1;
    initialWriteTime = 0.0;
    mirrorBCflag = 0;
    writeSparseActivity = false;
    InitVType = "ZeroV";
    VThresh = -infinity;   
    inFilename = "input/train/input.txt";
    phase = 0;
};

MLPForwardLayer "ForwardLayer1" = {
    restart = 0;
    nxScale = 1;
    nyScale = 1;
    nf = 2;
    writeStep = -1; //Change based on display period
    initialWriteTime = 0.0; //Change based on display period 
    mirrorBCflag = 0;
    writeSparseActivity = 0;
    InitVType = "ZeroV";
    VThresh = -infinity;
    phase = 1;
};

MLPSigmoidLayer "HiddenLayer1" = {  // 2
    nxScale           = 1.;
    nyScale           = 1.;
    nf                = 2;
    originalLayerName = "ForwardLayer1";
    InitVType         = "ZeroV";
    valueV            = 0;
    mirrorBCflag      = 0.0; 
    restart           = 0.0;      // from graylast
    linAlpha            = 0;
    spikingFlag       = 0.0;
    writeStep         = -1;
    writeNonspikingActivity = false;
    phase = 2;
};

MLPForwardLayer "ForwardLayer2" = {
   #include "ForwardLayer1";
   @nxScale = 1;
   @nyScale = 1;
   @nf = 1;
   @phase = 3;
};

MLPOutputLayer "FinalLayer" = {  // 2
    nxScale           = 1; //Doing 2 by 2 since this will test multiprocess as well
    nyScale           = 1;
    nf                = 1;
    originalLayerName = "ForwardLayer2";
    InitVType         = "ZeroV";
    valueV            = 0;
    mirrorBCflag      = 0.0; 
    linAlpha            = 0;
    restart           = 0.0;      // from graylast
    spikingFlag       = 0.0;
    writeStep         = -1;
    writeNonspikingActivity = false;
    //Local target in training will be different than local target in testing: this will test that each x/y network gets the same answer
    localTarget = false; //Reducing across all output layers
    phase = 4;
};

GTLayer "gt" = {
    restart = 0;
    nxScale = 1; 
    nyScale = 1;
    nf = 1;
    writeStep = -1;
    initialWriteTime = 0.0;
    mirrorBCflag = 0;
    writeSparseActivity = false;
    InitVType = "ZeroV";
    VThresh = -infinity;   
    inFilename = "input/train/gt.txt";
    phase = 3;
};

MLPErrorLayer "Error1" = {
    restart = 0;
    nxScale = 1;
    nyScale = 1;
    nf = 2;
    writeStep = -1; //Change based on display period
    mirrorBCflag = 0;
    writeSparseActivity = 0;
    linAlpha            = 0;
    ForwardLayername = "ForwardLayer1";
    InitVType = "ZeroV";
    VThresh = -infinity;
    phase = 6;
};

MLPErrorLayer "FinalError" = {
   #include "Error1";
   @nxScale = 1;
   @nyScale = 1;
   @nf = 1;
   @phase = 5;
   @ForwardLayername = "ForwardLayer2";
};

//Connections
KernelConn "W1" = {
    preLayerName = "train";
    postLayerName = "Error1";
    channelCode = -1; //Does not update on this channel
    nxp = 1; 
    nyp = 1; 
    nfp = 2;
    numAxonalArbors = 1;
    initFromLastFlag = 0;  // 1;  // restart
    writeStep = -1;
    
    weightInitType = "UniformRandomWeight";
    wMinInit = -1.225; //sqrt(3/2);
    wMaxInit = 1.225;
        
    strength = 1.0;  
    normalizeMethod = "none";
    
    shrinkPatches = false;
    //writeCompressedWeights = 0.0;
    writeCompressedCheckpoints = false;
    plasticityFlag = 1;
    weightUpdatePeriod = 1.0;
    initialWeightUpdateTime = 1.0;
    dWMax = .1; // 200.0 used for initial training 
    delay = 0;
     
    preActivityIsNotRate = false;
    selfFlag = false;

    updateGSynFromPostPerspective = false;
    pvpatchAccumulateType = "convolve";
};

CloneKernelConn "W1Clone" = {
    preLayerName = "train";
    postLayerName = "ForwardLayer1";
    channelCode = 0; //On exc channel
    writeStep = -1;
    originalConnName = "W1";
    selfFlag = false;
    delay = 0;
    preActivityIsNotRate = false;
    useWindowPost = false;
    updateGSynFromPostPerspective = false;
    pvpatchAccumulateType = "convolve";
};

KernelConn "B1" = {
    preLayerName = "onesLayer";
    postLayerName = "Error1";
    channelCode = -1; //Does not update on this channel
    nxp = 1; 
    nyp = 1; 
    nfp = 2;
    numAxonalArbors = 1;
    initFromLastFlag = 0;  // 1;  // restart
    writeStep = -1;
    
    weightInitType = "UniformRandomWeight";
    wMinInit = -1.225; //sqrt(3/2);
    wMaxInit = 1.225;
        
    strength = 1.0;  
    normalizeMethod = "none";
    
    shrinkPatches = false;
    //writeCompressedWeights = 0.0;
    writeCompressedCheckpoints = false;
    plasticityFlag = 1;
    weightUpdatePeriod = 1.0;
    initialWeightUpdateTime = 1.0;
    dWMax = .1; // 200.0 used for initial training 
    delay = 0;
     
    preActivityIsNotRate = false;
    selfFlag = false;

    updateGSynFromPostPerspective = false;
    pvpatchAccumulateType = "convolve";
};

CloneKernelConn "B1Clone" = {
    preLayerName = "onesLayer";
    postLayerName = "ForwardLayer1";
    channelCode = 0; //On exc channel
    writeStep = -1;
    originalConnName = "B1";
    selfFlag = false;
    delay = 0;
    preActivityIsNotRate = false;
    useWindowPost = false;
    updateGSynFromPostPerspective = false;
    pvpatchAccumulateType = "convolve";
};

KernelConn "W2" = {
   #include "W1";
   @nxp = 1;
   @nyp = 1;
   @nfp = 1;
   @preLayerName = "HiddenLayer1";
   @postLayerName = "FinalError";
   @wMinInit = -1.732; //sqrt(3/1);
   @wMaxInit = 1.732;
};

CloneKernelConn "W2Clone" = {
   #include "W1Clone";
   @preLayerName = "HiddenLayer1";
   @postLayerName = "ForwardLayer2";
   @originalConnName = "W2";
};

TransposeConn "W2T" = {
    preLayerName = "FinalError";
    postLayerName = "Error1";
    channelCode = 0; //On excitatory channel
    originalConnName = "W2";
    selfFlag = false;
    preActivityIsNotRate = false;  // should help make response more indepenent of time step size dt
    writeStep = -1;
    writeCompressedCheckpoints = false;
    delay = 0;
    pvpatchAccumulateType = "convolve";
    updateGSynFromPostPerspective = false;
};

KernelConn "B2" = {
   #include "B1";
   @postLayerName = "FinalError";
   @nfp = 1;
   @wMinInit = -1.732; //sqrt(3/1);
   @wMaxInit = 1.732;
};

CloneKernelConn "B2Clone" = {
   #include "B1Clone";
   @postLayerName = "ForwardLayer2";
   @originalConnName = "B2";
};

KernelConn "gtToFinalError" = {
    preLayerName = "gt";
    postLayerName = "FinalError";
    channelCode = 0; //Excitatory channel
    nxp = 1; 
    nyp = 1; 
    numAxonalArbors = 1;
    initFromLastFlag = 0;  // 1;  // restart
    writeStep = -1;
    writeCompressedCheckpoints = false;
    weightInitType = "UniformWeight";
    weightInit = 1; 
    normalizeMethod = "none";
    shrinkPatches = false;
    //writeCompressedWeights = 0.0;
    plasticityFlag = 0;
    pvpatchAccumulateType = "convolve";
    updateGSynFromPostPerspective = false;     
    delay = 0;
    preActivityIsNotRate = false;
    selfFlag = false;
};

IdentConn "FinalLayerToFinalError" = {
    preLayerName = "FinalLayer";
    postLayerName = "FinalError";
    channelCode = 1; //Inhibitory Channel
    writeStep = -1;    
    delay = 0;
};
